{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Paweł Chruszczewski\n",
    "\n",
    "Objective: As part of the study, I tried to determine whether the inclusion of the structural probability of default from the Merton model (PD) as an explanatory variable in classification models would improve the prediction of bankruptcy of the company. To conduct the study, I used the balance sheet data and quotations of American companies from the non-financial sector for the years 1985–2019 taken from the Compustat database. I obtained information on corporate insolvency from the UCLA LoPucki database. On the basis of the obtained results, there are no grounds to claim that the inclusion of the PD variable obtained from the Merton model significantly improves the predictive abilities of the models. For some classifiers, the mean ROC-AUC of the model with the PD variable is only slightly higher than the models without the PD variable, and the tests performed showed no statistical similarity of the ROC-AUC models for the significance level of 0.01, 0.05, 0.1. Nevertheless, the conducted study and literature research suggest that the procedure of selecting variables and number of variables may have the greatest impact on the rejection of a hypothesis. For this reason, it is suggested to re-conduct the study based on the step-wise selecetion algorithm. Code using this algorithm is also available in this notebook.\n",
    "\n",
    "Codes and libraries: This project requires Python  3. I have Used python 3.9. The following Python libraries are also required:\n",
    "\n",
    "<li> numpy\n",
    "<li> pandas\n",
    "<li> warnings\n",
    "<li> matplotlib\n",
    "<li> scikit-learn\n",
    "<li> xgboost\n",
    "<li> scipy\n",
    "<li> seaborn\n",
    "<li> itertools\n",
    "<li> math\n",
    "<li> mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "## Plotting libraries\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "## Sklearn Libraries\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, \\\n",
    "            classification_report, recall_score, precision_recall_curve, roc_auc_score, precision_score, accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "## XGBoost Librarires\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# pickle library\n",
    "import pickle\n",
    "\n",
    "## Scipy Libraries\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.stats import f\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "#statistics\n",
    "from statistics import stdev \n",
    "\n",
    "#itertools\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "#mlxtend\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "#math\n",
    "import math\n",
    "\n",
    "# Define random state\n",
    "random_state = 2020\n",
    "np.random.seed(random_state)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.DeLong_Test as delong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.Bootstrap_Test as bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset & Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'y']\n",
    "y = data.loc[:, data.columns == 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing extreme values(inf) to the 0.01 percentile and 0.99 percentile\n",
    "def winsorize_all(predictors):\n",
    "    for col in predictors.columns: \n",
    "         predictors[col] = winsorize(predictors[col], limits=0.01)\n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = winsorize_all(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predictors distribution\n",
    "# for i, col in enumerate(X.columns):\n",
    "#     plt.figure(i)\n",
    "#     sns.countplot(x=col, data=X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform first split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, \n",
    "                                                y, \n",
    "                                                test_size=0.2, \n",
    "                                                stratify = y,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_pd = xtrain.loc[:,xtrain.columns == 'pd']\n",
    "xtest_pd = xtest.loc[:,xtest.columns == 'pd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_nopd = xtrain.loc[:,xtrain.columns != 'pd']\n",
    "xtest_nopd = xtest.loc[:,xtest.columns != 'pd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_smoothing = np.logspace(-1,1, num=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_grid = {'gaussiannb__var_smoothing': var_smoothing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_params = [0.1,0.2,0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_grid = {'quadraticdiscriminantanalysis__reg_param': qda_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the norm used in the penalization\n",
    "logreg_penalty = ['l1', 'l2', 'elasticnet', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse of regularization strength\n",
    "logreg_c = [0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to use in the optimization problem\n",
    "logreg_solver = ['newton-cg','liblinear', 'saga', 'lbfgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_weight = ['balanced', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_grid = {'logisticregression__penalty' : logreg_penalty,\n",
    "               'logisticregression__C' : logreg_c,\n",
    "               'logisticregression__solver' : logreg_solver,\n",
    "               'logisticregression__class_weight': logreg_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion to split on\n",
    "dt_criterion = ['gini', 'entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The strategy used to choose the split at each node\n",
    "# dt_splitter = ['best', 'random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of levels in tree\n",
    "dt_max_depth = [int(x) for x in np.linspace(1, 20, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the default as a possible value\n",
    "dt_max_depth.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The minimum number of samples required to split an internal node\n",
    "# dt_min_samples_split = [int(x) for x in np.linspace(2, 40, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_min_impurity_decrease = [float(x) for x in np.linspace(0, 0.3, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The minimum number of samples required to be at a leaf node\n",
    "dt_min_samples_leaf = [int(x) for x in np.linspace(1, 5, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features to consider at every split\n",
    "dt_max_features = ['auto', 'sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights associated with classes\n",
    "dt_class = ['balanced_subsample', 'balanced', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid = {'decisiontreeclassifier__criterion': dt_criterion,\n",
    "#            'decisiontreeclassifier__splitter': dt_splitter,\n",
    "           'decisiontreeclassifier__max_depth': dt_max_depth,\n",
    "#            'decisiontreeclassifier__min_samples_split': dt_min_samples_split,\n",
    "#            'decisiontreeclassifier__min_impurity_decrease': dt_min_impurity_decrease,\n",
    "           'decisiontreeclassifier__min_samples_leaf':dt_min_samples_leaf,\n",
    "           'decisiontreeclassifier__max_features':dt_max_features,\n",
    "           'decisiontreeclassifier__class_weight': dt_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in Random Forest\n",
    "rf_n_estimators = [int(x) for x in np.linspace(100, 300, 3)]\n",
    "rf_n_estimators.append(10)\n",
    "rf_n_estimators.append(50)\n",
    "rf_n_estimators.append(1000)\n",
    "rf_n_estimators.append(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_max_depth = [int(x) for x in np.linspace(1, 20, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_max_depth.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_max_features = ['auto', 'sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_min_samples_leaf = [int(x) for x in np.linspace(1, 5, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_criterion = ['gini', 'entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_min_samples_split = [int(x) for x in np.linspace(2, 40, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_min_impurity_decrease = [float(x) for x in np.linspace(0, 0.3, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method of selecting samples for training each tree\n",
    "# rf_bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_class = ['balanced_subsample', 'balanced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {'randomforestclassifier__n_estimators': rf_n_estimators,\n",
    "           'randomforestclassifier__max_depth': rf_max_depth,\n",
    "           'randomforestclassifier__max_features': rf_max_features,\n",
    "           'randomforestclassifier__criterion': rf_criterion,\n",
    "#            'randomforestclassifier__min_samples_split': rf_min_samples_split,\n",
    "#            'randomforestclassifier__min_impurity_decrease': rf_min_impurity_decrease,\n",
    "           'randomforestclassifier__min_samples_leaf':rf_min_samples_leaf,\n",
    "#            'randomforestclassifier__bootstrap': rf_bootstrap,\n",
    "           'randomforestclassifier__class_weight': rf_class\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of levels in tree\n",
    "adab_max_depth = [1,2,5,10,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees to be used\n",
    "adab_n_estimators = [20,50,100,200,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "adab_eta = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab_algorithm = ['SAMME', 'SAMME.R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab_grid = {'adaboostclassifier__base_estimator__max_depth': adab_max_depth,\n",
    "             'adaboostclassifier__n_estimators': adab_n_estimators,\n",
    "             'adaboostclassifier__learning_rate': adab_eta,\n",
    "             'adaboostclassifier__algorithm': adab_algorithm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab = AdaBoostClassifier(base_estimator = RandomForestClassifier(random_state=random_state, class_weight = 'balanced'), random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees to be used\n",
    "xgb_n_estimators = [20,50,100,200,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of levels in tree\n",
    "xgb_max_depth = [1,2,5,10,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of instaces needed in each node\n",
    "xgb_min_child_weight = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree construction algorithm used in XGBoost\n",
    "xgb_tree_method = ['auto', 'exact', 'approx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "xgb_eta = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum loss reduction required to make further partition\n",
    "xgb_gamma = [x for x in np.linspace(0, 0.5, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning objective used\n",
    "# xgb_objective = ['binary:logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_lambda = [10,20,50,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing of positive and negative weights\n",
    "xgb_weight = [119.85522788203754, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_colsample_bytree = [x for x in np.linspace(0.1, 1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_bytree = [x for x in np.linspace(0.1, 1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "51002/426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = {'xgbclassifier__n_estimators': xgb_n_estimators,\n",
    "            'xgbclassifier__max_depth': xgb_max_depth,\n",
    "            'xgbclassifier__min_child_weight': xgb_min_child_weight,\n",
    "            'xgbclassifier__tree_method': xgb_tree_method,\n",
    "            'xgbclassifier__learning_rate': xgb_eta,\n",
    "            'xgbclassifier__gamma': xgb_gamma,\n",
    "#             'xgbclassifier__objective': xgb_objective,\n",
    "#             'xgbclassifier__reg_lambda':xgb_lambda,\n",
    "            'xgbclassifier__colsample_bytree':xgb_colsample_bytree,\n",
    "            'xgbclassifier__subsample_bytree':subsample_bytree,\n",
    "            'xgbclassifier__scale_pos_weight': xgb_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb =  XGBClassifier(random_state = random_state, objective = 'binary:logistic', scale_pos_weight = 119.72300469483568)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xtrain.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_leaf_size = [int(x) for x in np.linspace(1, 55, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_weights = ['uniform','distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_n_neighbors = [int(x) for x in np.linspace(1, 30, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_p= [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_metric = ['minkowski', 'euclidean', 'manhattan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid = {'kneighborsclassifier__leaf_size':knn_leaf_size,\n",
    "            'kneighborsclassifier__weights':knn_weights,\n",
    "            'kneighborsclassifier__n_neighbors':knn_n_neighbors,\n",
    "            'kneighborsclassifier__p': knn_p,\n",
    "            'kneighborsclassifier__metric': knn_metric}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity I choose 3 layers with the same number of neurons as there are features in my data set\n",
    "mlp_hidden_layer_sizes = [(26,26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_activation = ['tanh', 'relu', 'logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_solver = ['lbfgs', 'sgd', 'adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_alpha = np.linspace(0.0001,0.1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_eta = ['constant','invscaling','adaptive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid = {'mlpclassifier__hidden_layer_sizes': mlp_hidden_layer_sizes,\n",
    "            'mlpclassifier__activation': mlp_activation,\n",
    "            'mlpclassifier__solver': mlp_solver,\n",
    "            'mlpclassifier__alpha': mlp_alpha,\n",
    "            'mlpclassifier__learning_rate': mlp_eta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse of regularization strength\n",
    "svc_c = [0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel selects the type of hyperplane used to separate the data; \n",
    "# ‘linear’ will use a linear hyperplane (a line in the case of 2D data). ‘rbf’ and ‘poly’ uses a non linear hyper-plane;\n",
    "svc_kernel = ['linear', 'rbf', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when kernel set to ‘poly’, the degree of the polynomial used to find the hyperplane to split the data\n",
    "# svc_degree = [int(x) for x in np.linspace(0, 10, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter for non linear hyperplanes\n",
    "svc_gamma = [0.1, 1, 10, 100, 1000]\n",
    "svc_gamma.append('scale')\n",
    "svc_gamma.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_weight = ['balanced', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid = {'svc__C' : svc_c,\n",
    "            'svc__kernel':svc_kernel,\n",
    "#             'svc__degree':svc_degree,\n",
    "            'svc__gamma':svc_gamma,\n",
    "            'svc__class_weight':svc_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state = random_state, probability = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_classifier(object):\n",
    "    def __init__(self, n_splits, base_models, grids):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            n_splits: number of folds in k-fold cross-validation\n",
    "            base_models: List with set of classifiers\n",
    "            grids: list with set of parameters grids for classifiers\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.base_models = base_models\n",
    "        self.grids = grids\n",
    "\n",
    "    def predict(self, x_train, y_train, x_test, y_test, chosen_set = ''):\n",
    "        \"\"\"\n",
    "        The function normalizes predictors, searches hyperparameters space, chooses best set of predictors, using principal component analysis and predicts the results for best set of classifiers\n",
    "        Parameters:\n",
    "            x_train: training set; dataframe with predictors as columns\n",
    "            y_train: training set; dataframe with dependent variable as column\n",
    "            x_test: test set; dataframe with predictors as columns\n",
    "            y_test: test set; dataframe with dependent variable as column\n",
    "            chosen_set: designation of the data set for which the model is created: 'ALL' - all predictors, 'PDE' - all predictors, excluding PD, 'PD' - only PD predictor\n",
    "\n",
    "            return:\n",
    "                roc_auc_scores: metric on the basis of which the classifier is assessed, from sklearn.metrics.roc_auc_score\n",
    "                test_pred: dataframe with predictions of the classifiers as floats of the probability of being class 1\n",
    "                classifiers: list of best classifiers' instances fitted to the model\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state = random_state)\n",
    "                  \n",
    "        roc_auc_scores = pd.DataFrame(columns = [str(i).split('(')[0].lower() for i in self.base_models])\n",
    "        test_pred = pd.DataFrame(np.zeros((x_test.shape[0], len(self.base_models))), columns=[str(i).split('(')[0].lower() for i in self.base_models])\n",
    "        test_pred.columns = pd.MultiIndex.from_product([[chosen_set], test_pred.columns])\n",
    "        feat_selected = pd.DataFrame(np.empty((len(x_train.columns), len(self.base_models))), dtype = np.str, columns=[str(i).split('(')[0].lower() for i in self.base_models])\n",
    "        feat_selected.columns = pd.MultiIndex.from_product([[chosen_set], feat_selected.columns])\n",
    "        classifiers = []        \n",
    "        \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "        \n",
    "            pipe_lr = make_pipeline(PowerTransformer(method='yeo-johnson',standardize = True),\n",
    "                                    PCA(n_components = 0.99),\n",
    "                                    clf)\n",
    "            \n",
    "            search = RandomizedSearchCV(estimator=pipe_lr, param_distributions = self.grids[i], cv = cv, n_jobs=-1, verbose=True, scoring = 'roc_auc', iid = True, refit = True, n_iter = 50)\n",
    "\n",
    "            search.fit(x_train, y_train)\n",
    "            \n",
    "            filename = 'model_'+str(chosen_set)+str(clf).split('(')[0].lower()+'_saved.sav'\n",
    "            pickle.dump(pipe_lr.steps[2][1], open(filename, 'wb'))\n",
    "            \n",
    "            classifiers.append([chosen_set, i, search.best_estimator_.steps[2][1]])\n",
    "            \n",
    "            predict_rdf = search.best_estimator_.predict_proba(x_test)[:,1]\n",
    "            test_pred[chosen_set][str(clf).split('(')[0].lower()] = predict_rdf.astype('float64')\n",
    "                  \n",
    "            roc_auc_scores.loc[0,str(clf).split('(')[0].lower()] = roc_auc_score(y_test, predict_rdf)\n",
    "                \n",
    "        return roc_auc_scores, test_pred, classifiers\n",
    "    \n",
    "    def joined_scores(self, predict_df):\n",
    "        \"\"\"\n",
    "        The function joints the predictions of the predict function\n",
    "        Parameters\n",
    "            predict_df: list with dataframes; each dataframe in a list contains predictions of the classifiers as floats of the probability of being class 1\n",
    "\n",
    "            return:\n",
    "                predict_df_all: dataframe with predictions of the classifiers for all three datasets ('ALL', 'PDE', 'PD')\n",
    "                \n",
    "        \"\"\"\n",
    "            predict_df_all = pd.concat(predict_df, axis = 1)\n",
    "            \n",
    "            return predict_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Create_classifier(object):\n",
    "#     def __init__(self, n_splits, base_models, grids):\n",
    "#         self.n_splits = n_splits\n",
    "#         self.base_models = base_models\n",
    "#         self.grids = grids\n",
    "\n",
    "#     def predict(self, x_train, y_train, x_test, y_test, chosen_set = ''):\n",
    "\n",
    "          \"\"\"\n",
    "            The function normalizes predictors, searches hyperparameters space, chooses best set of predictors, using forward-selection algorithm and predicts the results for best set of classifiers\n",
    "            Parameters:\n",
    "                x_train: training set; dataframe with predictors as columns\n",
    "                y_train: training set; dataframe with dependent variable as column\n",
    "                x_test: test set; dataframe with predictors as columns\n",
    "                y_test: test set; dataframe with dependent variable as column\n",
    "                chosen_set: designation of the data set for which the model is created: 'ALL' - all predictors, 'PDE' - all predictors, excluding PD, 'PD' - only PD predictor\n",
    "\n",
    "                return:\n",
    "                    roc_auc_scores: metric on the basis of which the classifier is assessed, from sklearn.metrics.roc_auc_score\n",
    "                    test_pred: dataframe with predictions of the classifiers as floats of the probability of being class 1\n",
    "                    classifiers: list of best classifiers' instances fitted to the model\n",
    "\n",
    "         \"\"\"\n",
    "        \n",
    "#         cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state = random_state)\n",
    "                  \n",
    "#         roc_auc_scores = pd.DataFrame(columns = [str(i) for i in self.base_models])\n",
    "#         test_pred = pd.DataFrame(np.zeros((x_test.shape[0], len(self.base_models))), columns=[str(i).split('(')[0].lower() for i in self.base_models])\n",
    "#         test_pred.columns = pd.MultiIndex.from_product([[chosen_set], test_pred.columns])\n",
    "#         feat_selected = pd.DataFrame(np.zeros((len(x_train.columns), len(self.base_models))), index=x_train.columns, columns=[str(i) for i in self.base_models])\n",
    "#         feat_importance = pd.DataFrame(np.zeros((len(x_train.columns), len(self.base_models))), index=x_train.columns, columns=[str(i) for i in self.base_models])\n",
    "#         estimators = []         \n",
    "        \n",
    "#         for i, clf in enumerate(self.base_models):\n",
    "        \n",
    "#             pipe_lr = make_pipeline(PowerTransformer(method='yeo-johnson',standardize = True),\n",
    "#                                     SFS(estimator=clf, k_features='best', forward=True, floating=False, scoring='roc_auc',cv=cv),\n",
    "#                                     clf)\n",
    "            \n",
    "#             search = RandomizedSearchCV(estimator=pipe_lr, param_distributions = self.grids[i], cv = cv, n_jobs=-1, verbose=True, scoring = 'roc_auc', iid = True, refit = True, n_iter = 100)\n",
    "#             search = search.fit(x_train, y_train)\n",
    "\n",
    "#             search.fit(x_train, y_train)\n",
    "            \n",
    "#             filename = 'model_'+str(chosen_set)+str(clf)+'_saved.sav'\n",
    "#             pickle.dump(search.best_estimator_.steps[1][1], open(filename, 'wb'))\n",
    "            \n",
    "#             estimators.append([chosen_set, i, search.best_estimator_.steps[1][1]])\n",
    "            \n",
    "#             predict_rdf = search.best_estimator_.predict_proba(x_test)[:,1]\n",
    "#             test_pred[chosen_set][str(clf).split('(')[0].lower()] = predict_rdf.astype('float64')\n",
    "                  \n",
    "#             roc_auc_scores.loc[0,str(clf)] = roc_auc_score(y_test, predict_rdf)\n",
    "            \n",
    "#             feat_est = search.best_estimator_.steps[1][1].k_feature_idx_\n",
    "            \n",
    "#             for j in feat_est:\n",
    "#                 feat_selected.iloc[j,i] = 1\n",
    "                  \n",
    "#             for j in x_train.columns:\n",
    "#                 feat_est = dict(zip(x_train.columns, search.best_estimator_.steps[1][1].k_feature_idx_))\n",
    "#                 feat_selected.loc[str(j), str(clf)] = feat_est[str(j)]\n",
    "                \n",
    "#                 try:\n",
    "#                     importances = dict(zip(x_train.columns, search.best_estimator_.named_steps[str(clf).split('(')[0].lower()].feature_importances_))\n",
    "#                     feat_importance.loc[str(j), str(clf)] = importances[str(j)]\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "                \n",
    "#         return roc_auc_scores, test_pred, estimators\n",
    "    \n",
    "#     def joined_scores(self, predict_df):\n",
    "        \"\"\"\n",
    "        The function joints the predictions of the predict function\n",
    "        Parameters\n",
    "            predict_df: list with dataframes; each dataframe in a list contains predictions of the classifiers as floats of the probability of being class 1\n",
    "\n",
    "            return:\n",
    "                test_pred: dataframe with predictions of the classifiers for all three sets of predictors ('ALL', 'PDE', 'PD')\n",
    "                \n",
    "        \"\"\"\n",
    "#             roc_auc_all = pd.concat(self.roc_auc)\n",
    "#             predict_df_all = pd.concat(predict_df, axis = 1)\n",
    "#             return predict_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scoring(object):\n",
    "    def __init__(self, base_models):\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def delong_test(self, predict_df_all, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes p-value of DeLong Test with hypothesis that ROC AUCs of two classifiers are different\n",
    "        Parameters:\n",
    "            predict_df_all: dataframe with predictions of the classifiers for all three sets of predictors ('ALL', 'PDE', 'PD')\n",
    "            labels: test set; dataframe with dependent variable as column\n",
    "\n",
    "            return: \n",
    "                Test_df_sets: dataframe with results of paired test comparing ROC AUCs of different sets ('ALL' vs 'PDE' and 'ALL' vs 'PD') for all classifiers\n",
    "                Test_df_all: dataframe with results of paired test comparing ROC AUCs for all classifiers built based on set of all variables ('ALL')\n",
    "        \"\"\"\n",
    "\n",
    "        Test_df_sets = pd.DataFrame(np.zeros((2, len(self.base_models))), index=['ALL/PDE','ALL/PD'], columns=[str(i).split('(')[0].lower() for i in self.base_models])\n",
    "        Test_df_sets.columns = pd.MultiIndex.from_product([['DeLong Test'], Test_df_sets.columns])\n",
    "        \n",
    "        Test_df_all = pd.DataFrame(list(combinations(Test_df_sets['DeLong Test'].columns,2)),columns = ['1st Algorithm', '2nd Algorithm'])\n",
    "        Test_df_all['score'] = 0\n",
    "        Test_df_all.columns = pd.MultiIndex.from_product([['DeLong Test'], Test_df_all.columns])    \n",
    "            \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "        \n",
    "            Test_df_sets['DeLong Test'].loc['ALL/PDE',str(clf).split('(')[0].lower()] = delong.delong_roc_test(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PDE'][str(clf).split('(')[0].lower()])\n",
    "            Test_df_sets['DeLong Test'].loc['ALL/PD',str(clf).split('(')[0].lower()] = delong.delong_roc_test(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PD'][str(clf).split('(')[0].lower()])\n",
    "        \n",
    "        for j in range(Test_df_all.shape[0]):\n",
    "            Test_df_all.loc[j, ('DeLong Test', 'score')]  = delong.delong_roc_test(labels.values.ravel(), predict_df_all['ALL'][Test_df_all.loc[j, ('DeLong Test', '1st Algorithm')]], predict_df_all['ALL'][Test_df_all.loc[j, ('DeLong Test', '2nd Algorithm')]])\n",
    "       \n",
    "        return Test_df_sets, Test_df_all\n",
    "    \n",
    "    def bootstrap_test(self, predict_df_all, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes p-value of Bootstrap Test with hypothesis that ROC AUCs of two classifiers are different\n",
    "        Parameters:\n",
    "            predict_df_all: dataframe with predictions of the classifiers for all three sets of predictors ('ALL', 'PDE', 'PD')\n",
    "            labels: test set; dataframe with dependent variable as column\n",
    "\n",
    "            return: \n",
    "                Test_df_sets: dataframe with results of paired test comparing ROC AUCs of different sets ('ALL' vs 'PDE' and 'ALL' vs 'PD') for all classifiers\n",
    "                Test_df_all: dataframe with results of paired test comparing ROC AUCs for all classifiers built based on set of all variables ('ALL')\n",
    "        \"\"\"\n",
    "        \n",
    "        Test_df_sets = pd.DataFrame(np.zeros((2, len(self.base_models))), index=['ALL/PDE','ALL/PD'], columns=[str(i).split('(')[0].lower() for i in self.base_models])\n",
    "        Test_df_sets.columns = pd.MultiIndex.from_product([['Bootstrap Test'], Test_df_sets.columns])\n",
    "        \n",
    "        Test_df_all = pd.DataFrame(list(combinations(Test_df_sets['Bootstrap Test'].columns,2)),columns = ['1st Algorithm', '2nd Algorithm'])\n",
    "        Test_df_all['score'] = 0\n",
    "        Test_df_all.columns = pd.MultiIndex.from_product([['Bootstrap Test'], Test_df_all.columns])\n",
    "            \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "        \n",
    "            Test_df_sets['Bootstrap Test'].loc['ALL/PDE',str(clf).split('(')[0].lower()] = bootstrap.pvalue(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PDE'][str(clf).split('(')[0].lower()], score_fun=roc_auc_score)\n",
    "            Test_df_sets['Bootstrap Test'].loc['ALL/PD',str(clf).split('(')[0].lower()] = bootstrap.pvalue(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PD'][str(clf).split('(')[0].lower()], score_fun=roc_auc_score)\n",
    "            \n",
    "        for j in range(Test_df_all.shape[0]):\n",
    "            Test_df_all.loc[j, ('Bootstrap Test', 'score')]  = bootstrap.pvalue(labels.values.ravel(), predict_df_all['ALL'][Test_df_all.loc[j, ('Bootstrap Test', '1st Algorithm')]], predict_df_all['ALL'][Test_df_all.loc[j, ('Bootstrap Test', '2nd Algorithm')]],score_fun=roc_auc_score)\n",
    "       \n",
    "        return Test_df_sets, Test_df_all\n",
    "    \n",
    "    def likelihood_RT(self, predict_df_all, estimators, x_train, y_test):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes p-value of Likelihood Ratio Test with hypothesis that ROC AUCs of Logistic Regression models built on different sets of variables are different\n",
    "        Parameters:\n",
    "            predict_df_all: dataframe with predictions of the classifiers for all three sets of predictors ('ALL', 'PDE', 'PD')\n",
    "            classifiers: list of best classifiers' instances fitted to the model\n",
    "            x_train: training set; dataframe with predictors as columns\n",
    "            labels: test set; dataframe with dependent variable as column\n",
    "\n",
    "            return: \n",
    "                Test_df_sets: dataframe with results of paired test comparing ROC AUCs of different sets ('ALL' vs 'PDE' and 'ALL' vs 'PD') for all classifiers\n",
    "        \"\"\"\n",
    "        \n",
    "        Test_df_sets = pd.DataFrame((np.zeros((2, 1))), index=['ALL/PDE','ALL/PD'], columns=[str(classifiers[2])])\n",
    "        Test_df_sets.columns = pd.MultiIndex.from_product([['LRT'], Test_df_sets.columns])\n",
    "\n",
    "        alt_log_likelihood = -log_loss(y_test,\n",
    "                                       predict_df_all['ALL'][str(classifiers[2])],\n",
    "                                       normalize=False)\n",
    "        null_log_likelihood = -log_loss(y_test,\n",
    "                                        predict_df_all['PDE'][str(classifiers[2])],\n",
    "                                        normalize=False)\n",
    "        G = 2 * (alt_log_likelihood - null_log_likelihood)\n",
    "        p_log_l = chi2.sf(G, x_train.shape[1])\n",
    "        \n",
    "        alt_log_likelihood = -log_loss(y_test,\n",
    "                                       predict_df_all['ALL'][str(classifiers[2])],\n",
    "                                       normalize=False)\n",
    "        null_log_likelihood = -log_loss(y_test,\n",
    "                                        predict_df_all['PD'][str(classifiers[2])],\n",
    "                                        normalize=False)\n",
    "        \n",
    "        G = 2 * (alt_log_likelihood - null_log_likelihood)\n",
    "        p_log_2 = chi2.sf(G, x_train.shape[1])\n",
    "        \n",
    "        Test_df_sets['LRT'].loc['ALL/PDE' ,str(classifiers[2])] = p_log_l\n",
    "        Test_df_sets['LRT'].loc['ALL/PD', str(classifiers[2])] = p_log_2\n",
    "\n",
    "        return Test_df_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yeoj_graph(x_train, lbd_list, feature=''):\n",
    "    \n",
    "    \"\"\"\n",
    "    Normalization of a selected predictor using the Yeo-Johnson transformation for various λ parameters.\n",
    "    Parameters:\n",
    "        x_train: training set; dataframe with predictors as columns\n",
    "        lbd_list: list of λ parameters for which the transofrmation is conducted\n",
    "        feature: string name of predictor\n",
    "\n",
    "        return: Graph showing the non-transformed against transformed values of predictor for various λ parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    for i in range(len(lbd_list)):\n",
    "        n_lines = len(lbd_list)\n",
    "        c = np.arange(1, n_lines + 1)\n",
    "        norm = mpl.colors.Normalize(vmin=c.min(), vmax=c.max())\n",
    "        cmap = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.Greys)\n",
    "        cmap.set_array([])\n",
    "        a = x_train[feature].values.ravel()\n",
    "        a = np.sort(a)\n",
    "        b = yeojohnson(x_train[feature], lmbda=lbd_list[i])\n",
    "        b = np.sort(b)\n",
    "        plt.plot(a,b, c=cmap.to_rgba(i + 1), label='λ = '+str(lbd_list[i]))\n",
    "    plt.legend(loc=0)\n",
    "    plt.ylabel(\"ψ(λ,x)\", fontsize=15)\n",
    "    plt.xlabel(\"x\", fontsize=15)\n",
    "    plt.savefig('yeo-johnson.png', dpi=1200)\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def roc_comparison_sets(predict_df_all, y_test, chosen_set = ''):\n",
    "    \n",
    "    \"\"\"\n",
    "    ROC Curves for given set of predictors\n",
    "    Parameters:\n",
    "        predict_df_all: dataframe with predictions of the classifiers for all three datasets ('ALL', 'PDE', 'PD')\n",
    "        y_test: test set; dataframe with dependent variable as column\n",
    "        chosen_set: designation of the data set for which the model is created: 'ALL' - all predictors, 'PDE' - all predictors, excluding PD, 'PD' - only PD predictor\n",
    "\n",
    "        return: Graph showing the ROC Curves for chosen set of predictors\n",
    "    \"\"\"\n",
    "    \n",
    "    predict_df = predict_df_all[str(chosen_set)]\n",
    "    \n",
    "    # Plot the figure\n",
    "    # Train the models and record the results\n",
    "    result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "    for i, (j, clf) in enumerate(predict_df):\n",
    "        yproba = predict_df[j][clf]\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test.values.ravel(),  yproba)\n",
    "        auc = roc_auc_score(y_test.values.ravel(), yproba)\n",
    "\n",
    "        result_table = result_table.append({'classifiers': [j,clf],\n",
    "                                            'fpr':fpr, \n",
    "                                            'tpr':tpr, \n",
    "                                            'auc':auc}, ignore_index=True)\n",
    "\n",
    "        result_table[['set', 'classifier']] = pd.DataFrame(result_table['classifiers'].tolist(), index=result_table.index)\n",
    "\n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "        for k,m in enumerate(result_table.set.unique()):\n",
    "\n",
    "            plt.plot(result_table.loc[(result_table.classifier == str(clf)) & (result_table.set == str(m))]['fpr'].values[0], \n",
    "                     result_table.loc[(result_table.classifier == str(clf)) & (result_table.set == str(m))]['tpr'].values[0],\n",
    "                     label=\"{}, AUC={:.3f}\".format(str(m), result_table.loc[(result_table.classifier == str(clf)) & (result_table.set == str(m))]['auc'].values[0]))\n",
    "\n",
    "            plt.plot([0,1], [0,1], color='gray', linestyle='--')\n",
    "\n",
    "            plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "            plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "            plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "            plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "            plt.legend(prop={'size':13}, loc='lower right')\n",
    "            \n",
    "        plt.savefig(str(clf)+str(m)+'.png',  dpi=1200)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_ci_alternative(predict_df_all, y_test, base_models):\n",
    "    \n",
    "    \"\"\"\n",
    "    Confidence Intervals for given set of predictors\n",
    "    Parameters:\n",
    "        predict_df_all: dataframe with predictions of the classifiers for all sets of predictors ('ALL', 'PDE', 'PD')\n",
    "        y_test: test set; dataframe with dependent variable as column\n",
    "        base_models: List with set of classifiers\n",
    "\n",
    "        return: Graph showing the Confidence Intervals of all classifiers for given set of predictors\n",
    "    \"\"\"\n",
    "    predict_df = predict_df_all.copy(deep=False)\n",
    "        \n",
    "    result_table = pd.DataFrame(columns=['classifiers', 'delong','bootstrap'])\n",
    "    for i, (j, clf) in enumerate(predict_df):\n",
    "        yproba = predict_df[j][clf]\n",
    "\n",
    "        delong = delong.calc_auc_ci(y_test.values.ravel(),  yproba, alpha=0.95) \n",
    "        bootstrap = bootstrap.score_stat_ci(y_test.values.ravel(), yproba,  roc_auc_score)\n",
    "\n",
    "        result_table = result_table.append({'classifiers': [j,clf],\n",
    "                                            'delong':delong, \n",
    "                                            'bootstrap':bootstrap}, ignore_index=True)\n",
    "\n",
    "        result_table[['set', 'classifier']] = pd.DataFrame(result_table['classifiers'].tolist(), index=result_table.index)\n",
    "\n",
    "    for k,m in enumerate(result_table.set.unique()):\n",
    "    \n",
    "        plt.figure(figsize=(8,6))\n",
    "\n",
    "        SMALL_SIZE = 10\n",
    "        MEDIUM_SIZE = 12\n",
    "        BIGGER_SIZE = 14\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "        x_ticks = [str(i).split('(')[0].lower() for i in base_models]\n",
    "\n",
    "        for n,l in enumerate(result_table.classifier.unique()):\n",
    "\n",
    "            eb_1 = plt.errorbar(x=n+1, \n",
    "                             y=(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][1] + result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][0])/2, \n",
    "                             yerr=[(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][1] - result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][0])/2],\n",
    "                             fmt='ok',\n",
    "                             capsize = 10)\n",
    "\n",
    "            eb_2 = plt.errorbar(x=n+1.1, \n",
    "                             y=(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][1] + result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][0])/2, \n",
    "                             yerr=[(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][1] - result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][0])/2],\n",
    "                             fmt='ok',\n",
    "                             capsize = 10)\n",
    "            eb_2[-1][0].set_linestyle('--')\n",
    "            \n",
    "            # I need to manipulate 3rd parameter in arange, so the graph looks nice & I also need to do the same in errorbar(x)\n",
    "\n",
    "            plt.xticks(np.arange(1.05,len(x_ticks)+0.5,1), x_ticks, rotation=90)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.ylabel(\"ROC AUC Przedział Ufności\", fontsize=15)\n",
    "            plt.tight_layout()\n",
    "\n",
    "        plt.savefig('plot'+str(m)+'ci.png', dpi=1200)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_set = ['ALL', 'PDE', 'PD']\n",
    "base_models = [gnb, qda, logreg, dt, rf, xgb, adab, knn, svc, mlp]\n",
    "n_splits = 4\n",
    "grids = [gnb_grid, qda_grid, logreg_grid, dt_grid, rf_grid, xgb_grid, adab_grid, knn_grid, svc_grid, mlp_grid]\n",
    "lgb_stack = Create_classifier(n_splits = n_splits, base_models = base_models, grids = grids)        \n",
    "roc_auc_scores, test_pred, classifiers = lgb_stack.predict(xtrain, ytrain, xtest, ytest, chosen_set = chosen_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = [test_pred, test_pred2, test_pred3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining results from three datasets\n",
    "predict_df_all = lgb_stack.joined_scores(predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_score = Scoring(base_models = base_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sets_bootstrap, df_all_bootstrap, df_sets_bootstrap = lgb_score.bootstrap_test(predict_df_all, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sets_delong, df_all_delong = lgb_score.delong_test(predict_df_all, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likelihood = lgb_score.likelihood_RT(predict_df_all, ytest, xtrain):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeo-johnson graph\n",
    "lbd_list = [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]\n",
    "yeoj_graph(xtrain, lbd_list, feature = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence intervals graphs\n",
    "graph_ci_alternative(predict_df_all, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc graphs comparing sets\n",
    "roc_comparison_sets(predict_df_all, ytest)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
