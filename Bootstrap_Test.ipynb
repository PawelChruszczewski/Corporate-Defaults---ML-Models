{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_stat_ci(\n",
    "    y_true,\n",
    "    y_preds,\n",
    "    score_fun,\n",
    "    stat_fun=np.mean,\n",
    "    n_bootstraps=2000,\n",
    "    confidence_level=0.95,\n",
    "    seed=None,\n",
    "    reject_one_class_samples=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "    Parameters:\n",
    "        y_true: 1D list or array of labels.\n",
    "        y_preds: A list of lists or 2D array of predictions corresponding to elements in y_true.\n",
    "        score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.accuracy_score)\n",
    "        stat_fun: Statistic for which confidence interval is computed. (e.g. np.mean)\n",
    "        n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "        confidence_level: Confidence level for computing confidence interval. (default: 0.95)\n",
    "        seed: Random seed for reproducibility. (default: None)\n",
    "        reject_one_class_samples: Whether to reject bootstrapped samples with only one label. For scores like AUC we need at least one positive and one negative sample. (default: True)\n",
    "        \n",
    "        return: lower confidence interval, upper confidence interval\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_preds = np.atleast_2d(y_preds)\n",
    "    assert all(len(y_true) == len(y) for y in y_preds)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    scores = []\n",
    "    for i in range(n_bootstraps):\n",
    "        readers = np.random.randint(0, len(y_preds), len(y_preds))\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        if reject_one_class_samples and len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        reader_scores = []\n",
    "        for r in readers:\n",
    "            reader_scores.append(score_fun(y_true[indices], y_preds[r][indices]))\n",
    "        scores.append(stat_fun(reader_scores))\n",
    "\n",
    "    mean_score = np.mean(scores)\n",
    "    sorted_scores = np.array(sorted(scores))\n",
    "    alpha = (1.0 - confidence_level) / 2.0\n",
    "    ci_lower = sorted_scores[int(round(alpha * len(sorted_scores)))]\n",
    "    ci_upper = sorted_scores[int(round((1.0 - alpha) * len(sorted_scores)))]\n",
    "    \n",
    "    ci_boot = np.array([ci_lower,ci_upper])\n",
    "    \n",
    "    return ci_boot\n",
    "\n",
    "def pvalue(\n",
    "    y_true,\n",
    "    y_preds1,\n",
    "    y_preds2,\n",
    "    score_fun,\n",
    "    stat_fun=np.mean,\n",
    "    n_bootstraps=2000,\n",
    "    seed=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute p-value for hypothesis that given statistic of score function for model I predictions is higher than for\n",
    "    model II predictions using bootstrapping.\n",
    "    Parameters:\n",
    "        y_true: 1D list or array of labels.\n",
    "        y_preds1: A 1D array of predictions for model I corresponding to elements in y_true.\n",
    "        y_preds2: A 1D array of predictions for model II corresponding to elements in y_true.\n",
    "        score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.roc_auc_score)\n",
    "        stat_fun: Statistic for which p-value is computed. (default : mean)\n",
    "        n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "        two_tailed: Whether to use two-tailed test. (default: True)\n",
    "        seed: Random seed for reproducibility. (default: None)\n",
    "        \n",
    "        return: Computed p-value, array of bootstrapped differences of scores.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    m = []\n",
    "    s1 = []\n",
    "    sd = []\n",
    "    score1 = []\n",
    "    score2 = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        readers1 = np.random.randint(0, len(y_preds1), len(y_preds1))\n",
    "        readers2 = np.random.randint(0, len(y_preds2), len(y_preds2))\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        score_1 = roc_auc_score(y_true[indices], y_preds1[indices])\n",
    "        score_2 = roc_auc_score(y_true[indices], y_preds2[indices])\n",
    "        s1.append(score_1-score_2)\n",
    "        score1.append(roc_auc_score(y_true[indices], y_preds1[indices]))\n",
    "        score2.append(roc_auc_score(y_true[indices], y_preds2[indices]))\n",
    "                \n",
    "    m.append(stat_fun(score1) - stat_fun(score2))\n",
    "    sd.append(stdev(s1))\n",
    "    \n",
    "    Z = m[0]/sd[0]\n",
    "    \n",
    "    p = norm.cdf(abs(Z))\n",
    "\n",
    "    return p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
